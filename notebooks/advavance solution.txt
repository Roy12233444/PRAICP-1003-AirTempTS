Nice — I’ll give you a clear, runnable **step-by-step** plan + Python code to:

1. **Fix feature alignment** (training ↔ inference) so no missing columns.
2. **Select the best saved pipeline** and validate it with time-series cross-validation and residual checks.
3. **Train & compare ARIMA and Prophet baselines**.
4. **Retrain the chosen model on full data and produce 12-month forecasts with intervals**.
5. **Show advanced model suggestions** (XGBoost, LightGBM, CatBoost, ensembles, deep models) and short code examples to try them.

All code uses your project paths (you gave earlier). Paste cells into a notebook or run as scripts from your project root:

`PROJECT_ROOT = r"E:\AI-Engineering-Capstone-Projects\AirTempTs\PRAICP-1003-AirTempTS"`

---

# 0. Prep — imports, paths, small helpers

```python
# run in notebook cell or script (project root)
import os, sys, json
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

PROJECT_ROOT = r"E:\AI-Engineering-Capstone-Projects\AirTempTs\PRAICP-1003-AirTempTS"
DATA_EXTENDED = os.path.join(PROJECT_ROOT, "data", "surface-air-temperature-monthly-mean-extended.csv")
DATA_RAW = os.path.join(PROJECT_ROOT, "data", "surface-air-temperature-monthly-mean.csv")
RESULTS_DIR = os.path.join(PROJECT_ROOT, "results")
MODELS_DIR = os.path.join(RESULTS_DIR, "models")
FORECASTS_CSV = os.path.join(RESULTS_DIR, "forecasts_final.csv")

# ensure src import works (if running in notebook)
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# util imports
from joblib import load, dump
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# plotting defaults
plt.rcParams["figure.figsize"] = (10,5)
```

---

# 1. Fix feature alignment (robust function)

Goal: load the saved pipeline, determine its expected input feature names, then reindex your inference dataframe to that exact column list (fill missing features with zeros or reasonable placeholder). This avoids placeholder-on-the-fly hacks and ensures repeatable inference.

```python
from joblib import load
import numpy as np

def load_pipeline(pipeline_path):
    p = load(pipeline_path)
    return p

def infer_expected_features(pipeline):
    """
    Try several common locations to read the feature names expected by the pipeline.
    Returns list of feature names (or None if cannot detect).
    """
    # candidate attributes we try to read in order
    candidates = []
    # top-level attribute
    for attr in ("feature_names_in_", "feature_names", "columns", "feature_names_out_"):
        candidates.append(attr)
    # if pipeline contains a preprocessor/column transformer
    if hasattr(pipeline, "named_steps"):
        for name, step in pipeline.named_steps.items():
            for attr in ("feature_names_in_", "columns", "feature_names"):
                candidates.append((name, attr))
    # try get_feature_names_out call
    try:
        cols = pipeline.get_feature_names_out()
        return list(cols)
    except Exception:
        pass

    # try attributes
    for c in candidates:
        try:
            if isinstance(c, tuple):
                stepname, attr = c
                step = pipeline.named_steps.get(stepname)
                if step is not None and hasattr(step, attr):
                    val = getattr(step, attr)
                    return list(val) if not callable(val) else list(val())
            else:
                if hasattr(pipeline, c):
                    val = getattr(pipeline, c)
                    return list(val) if not callable(val) else list(val())
        except Exception:
            continue
    return None

def align_features(df_in: pd.DataFrame, expected_cols: list, fill_value=0.0):
    """
    Return DataFrame reindexed to expected_cols, filling missing with fill_value.
    Also preserves order.
    """
    # only keep numeric columns for prediction side; attempt coerce numeric where sensible
    out = df_in.copy()
    # Try to coerce month column to datetime if present
    if 'month' in out.columns:
        try:
            out['month'] = pd.to_datetime(out['month'])
        except Exception:
            pass
    # ensure expected columns exist in DataFrame; reindex
    for col in expected_cols:
        if col not in out.columns:
            out[col] = fill_value
    out = out[expected_cols].copy()
    return out
```

**How to use (example)**

```python
pipeline_path = os.path.join(MODELS_DIR, "final_new_features_pipeline.joblib")
p = load_pipeline(pipeline_path)
expected = infer_expected_features(p)
print("Detected expected feature count:", None if expected is None else len(expected))

# load your extended csv (the features generator output)
df_ext = pd.read_csv(DATA_EXTENDED, parse_dates=['month'])
print("Extended df shape:", df_ext.shape)

if expected is None:
    print("Warning: could not auto-detect expected features; inspect pipeline manually.")
else:
    X_aligned = align_features(df_ext, expected, fill_value=0.0)
    print("Aligned X shape:", X_aligned.shape)
```

If `expected` is `None`, inspect the pipeline manually (print pipeline). If it returns a list, you can now safely call `p.predict(X_aligned)`.

---

# 2. Quick inference test using aligned features (sanity check)

```python
# quick predict test (first few rows)
y_pred = p.predict(X_aligned.iloc[-100:])  # small test slice
print("Sample predictions:", y_pred[:5])
```

If prediction raises errors about dtypes, coerce types appropriately (e.g., `X_aligned = X_aligned.astype(float)` for numeric columns; keep caution with categorical encoders).

---

# 3. Time-series cross-validation (rolling origin) on pipeline

Use `TimeSeriesSplit` to get robust estimate. We will re-train the pipeline inside the CV splits if pipeline includes training steps. If pipeline is already a fitted model, you can re-fit model class with same parameters — but easiest is to re-create training code that mirrors pipeline. If you don't have training function, you can evaluate the fitted model on multiple out-of-sample periods by training separate pipelines — below is a generic approach assuming you have a `train_fn(X_train, y_train)` that returns a fitted sklearn-like pipeline. If not, you can still evaluate performance stability by doing rolling predictions where you fit a simple model.

Here I provide a practical CV that refits a simple LightGBM regressor pipeline (fast) — replace with your own pipeline training steps as needed.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from lightgbm import LGBMRegressor  # install lightgbm if not present

def train_lgb_pipeline(X_train, y_train):
    pipe = Pipeline([
        ("scaler", StandardScaler()),
        ("lgb", LGBMRegressor(n_estimators=200, learning_rate=0.05, random_state=42))
    ])
    pipe.fit(X_train, y_train)
    return pipe

def time_series_cv(X, y, n_splits=5):
    tscv = TimeSeriesSplit(n_splits=n_splits)
    mses, rmses, r2s = [], [], []
    split_idx = 0
    for train_index, test_index in tscv.split(X):
        split_idx += 1
        Xtr, Xte = X.iloc[train_index], X.iloc[test_index]
        ytr, yte = y.iloc[train_index], y.iloc[test_index]
        model = train_lgb_pipeline(Xtr, ytr)
        yhat = model.predict(Xte)
        mse = mean_squared_error(yte, yhat)
        rmse = np.sqrt(mse)
        r2 = r2_score(yte, yhat)
        print(f"Split {split_idx}: RMSE={rmse:.4f}, R2={r2:.4f}")
        mses.append(mse); rmses.append(rmse); r2s.append(r2)
    return {'mse': np.mean(mses), 'rmse': np.mean(rmses), 'r2': np.mean(r2s)}

# Example: using aligned features and target column from extended CSV
y = df_ext['mean_temp'].reset_index(drop=True)
X = align_features(df_ext, expected, fill_value=0.0).reset_index(drop=True)

cv_res = time_series_cv(X, y, n_splits=5)
print("CV summary:", cv_res)
```

If you cannot import LightGBM, replace with `RandomForestRegressor` from `sklearn`.

---

# 4. Residual diagnostics for chosen (best) pipeline

Compute residuals on test set (the last `n_test` months you held out) and plot residual timeseries and ACF.

```python
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf

# assume you have y_test and y_pred for final model on holdout
# Example: use last 12 rows as holdout
n_test = 12
y_true = df_ext['mean_temp'].iloc[-n_test:].reset_index(drop=True)
X_test = align_features(df_ext, expected, fill_value=0.0).iloc[-n_test:].reset_index(drop=True)
y_pred = p.predict(X_test)

resid = y_true - y_pred
plt.figure()
plt.plot(df_ext['month'].iloc[-n_test:], resid, marker='o')
plt.title("Residuals (last {} months)".format(n_test))
plt.axhline(0, color='k', linewidth=0.8)
plt.show()

# ACF
plot_acf(resid, lags=12)
plt.show()

# residual stats
print("Residual mean:", resid.mean())
print("Residual std:", resid.std())
```

Look for auto-correlation in residuals. If residuals show structure → model missing systematic patterns (more features or different model needed).

---

# 5. ARIMA baseline (pmdarima.auto\_arima) and forecast with intervals

```python
import pmdarima as pmd

# load raw series (monthly)
df_raw = pd.read_csv(DATA_RAW, parse_dates=['month'])
series = df_raw.set_index('month')['mean_temp'].asfreq('MS')  # monthly start

# split train/test (last 12 months test)
train = series.iloc[:-12]
test = series.iloc[-12:]

# auto ARIMA fit on train
arima_model = pmd.auto_arima(train, seasonal=True, m=12, trace=True,
                            error_action='ignore', suppress_warnings=True, stepwise=True)
print("ARIMA order:", arima_model.order, "seasonal_order:", arima_model.seasonal_order)

# forecast
n_forecast = 12
fc, conf_int = arima_model.predict(n_periods=n_forecast, return_conf_int=True)
fc_index = pd.date_range(start=series.index.max() + pd.offsets.MonthBegin(1), periods=n_forecast, freq='MS')
fc_series = pd.Series(fc, index=fc_index)
conf_df = pd.DataFrame(conf_int, index=fc_index, columns=['lower', 'upper'])

# evaluate on test (if forecasting exactly test period)
# if test is available and aligned:
pred_test = pd.Series(arima_model.predict(n_periods=len(test)), index=test.index)
print("ARIMA RMSE on test:", np.sqrt(mean_squared_error(test, pred_test)))
```

Save ARIMA forecast + intervals:

```python
arima_out = pd.DataFrame({'month': fc_index, 'forecast': fc})
arima_out['lower'] = conf_df['lower'].values
arima_out['upper'] = conf_df['upper'].values
arima_out.to_csv(os.path.join(RESULTS_DIR, "forecasts_arima_full.csv"), index=False)
```

---

# 6. Prophet baseline (guarded, may require installation)

```python
try:
    from prophet import Prophet
    df_prop = df_raw.reset_index().rename(columns={'month':'ds','mean_temp':'y'})[['ds','y']]
    m = Prophet(yearly_seasonality=True, weekly_seasonality=False, daily_seasonality=False)
    m.fit(df_prop.iloc[:-12])  # fit on train
    future = m.make_future_dataframe(periods=12, freq='MS')
    forecast_prophet = m.predict(future)
    # extract forecast for last 12 and future 12
    fc = forecast_prophet[['ds','yhat','yhat_lower','yhat_upper']].set_index('ds').iloc[-12:]
    fc.reset_index().to_csv(os.path.join(RESULTS_DIR, "forecasts_prophet.csv"), index=False)
    print("Prophet forecast saved")
except Exception as e:
    print("Prophet not available or failed:", e)
    print("To install (prefer conda on Windows): conda install -c conda-forge prophet")
```

---

# 7. Retrain chosen pipeline on FULL data & produce final 12-month forecast

When you’re satisfied with a pipeline (e.g., `final_new_features_pipeline.joblib`), **retrain the same modeling recipe on the entire dataset** (no test holdout) so the model uses all information to forecast future months. If your saved pipeline is a fitted estimator only, you must re-run the training code (not just reuse the fitted object). If the saved `joblib` is a pipeline object that you can re-fit, do so.

Simplest approach: retrain a pipeline with the same model type (example shown for LightGBM pipeline). If you want to reuse the exact same preprocessing + model architecture from saved pipeline but retrain, you must have the training script that built it. Below is an example retrain for the LGB pipeline:

```python
# Prepare X_full, y_full
df_full = pd.read_csv(DATA_EXTENDED, parse_dates=['month'])
y_full = df_full['mean_temp'].reset_index(drop=True)
X_full = align_features(df_full, expected, fill_value=0.0).reset_index(drop=True)

# retrain pipeline (example LGB)
final_model = train_lgb_pipeline(X_full, y_full)  # from earlier function
# save model
final_model_path = os.path.join(MODELS_DIR, "final_lgb_full.joblib")
dump(final_model, final_model_path)
print("Saved final model to:", final_model_path)

# Forecast next 12 months: need to construct input rows for future months
# For pure ML models based on lag features, you must iteratively create future feature rows.
def iterative_forecast_ml(model, df_history, steps=12, expected_cols=None):
    """
    df_history: original df with month and mean_temp, most recent last.
    expected_cols: list of feature names expected by model
    Returns DataFrame with months and forecasts.
    """
    history = df_history.copy().reset_index(drop=True)
    preds = []
    months = []
    for step in range(steps):
        # build feature row for next month from history
        # reuse your feature creation function — here we use src.features.make_feature_matrix
        # but need to ensure it produces features for last timestamp + new placeholder month
        # Simplest iterative approach: append placeholder month with NaN mean_temp and recompute features
        next_month = history['month'].iloc[-1] + pd.offsets.MonthBegin(1)
        next_row = {'month': next_month, 'mean_temp': np.nan}
        history = history.append(next_row, ignore_index=True)
        feats = make_feature_matrix(history)   # this will drop rows without enough lags, be careful
        # take last row features and align
        last_feats = feats.iloc[[-1]].copy()
        X_row = align_features(last_feats, expected_cols, fill_value=0.0)
        yhat = model.predict(X_row)[0]
        # store prediction and fill history
        preds.append(yhat)
        months.append(next_month)
        history.loc[history.index[-1], 'mean_temp'] = yhat  # feed prediction forward
    return pd.DataFrame({'month': months, 'forecast': preds})

# run iterative forecast
final_forecast = iterative_forecast_ml(final_model, df_full[['month','mean_temp']], steps=12, expected_cols=expected)
final_forecast.to_csv(FORECASTS_CSV, index=False)
print("Final forecast saved to:", FORECASTS_CSV)
```

Notes:

* Iterative forecasting for ML models requires you to generate lag features that use previous predictions — the `iterative_forecast_ml` above is a simple pattern but you must ensure `make_feature_matrix` can create features for the appended rows. Adjust as needed.
* For ARIMA/Prophet you can directly forecast multi-step with confidence intervals.

---

# 8. Uncertainty for ML model (bootstrap ensemble)

Simple bootstrap method to create prediction intervals:

```python
import random
from joblib import Parallel, delayed

def bootstrap_predictions(train_df, train_y, test_row, n_boot=100, sample_frac=0.8):
    preds = []
    n = len(train_df)
    for i in range(n_boot):
        idx = np.random.choice(range(n), size=int(n*sample_frac), replace=True)
        Xs = train_df.iloc[idx]
        ys = train_y.iloc[idx]
        m = train_lgb_pipeline(Xs, ys)
        preds.append(m.predict(test_row)[0])
    return np.array(preds)

# compute bootstrap for each forecasted row (this can be slow)
# Example: compute for the first forecasted month
# preds_boot = bootstrap_predictions(X_full, y_full, X_row, n_boot=200)
# lower = np.percentile(preds_boot, 2.5); upper = np.percentile(preds_boot,97.5)
```

This gives empirical intervals; slower but easy.

---

# 9. Advanced algorithms to improve accuracy (recommendations + short code notes)

High-signal algorithms you can try (ordered by likely ROI):

* **LightGBM** (`lightgbm`) — fast gradient boosting, great for tabular/time-series features.
* **XGBoost** (`xgboost`) — popular GBM, robust.
* **CatBoost** (`catboost`) — handles categorical features natively; good default.
* **Ensemble / Stacking** — stack LightGBM + XGBoost + CatBoost + linear model (meta-learner).
* **RandomForest / ExtraTrees** — robust baseline.
* **Neural models**:

  * **LSTM/GRU** (PyTorch / Keras) — sequence modeling if you feed windows.
  * **TemporalFusionTransformer** via `pytorch-forecasting` — state-of-the-art but heavier.
* **Gaussian Process Regression** — useful for uncertainty, but scales poorly.
* **TFT / Transformer-based models** via `pytorch-forecasting` or `darts`.

Short example to train LightGBM with simple hyperparams (we used earlier `train_lgb_pipeline`). For hyperparameter tuning, try **Optuna** (install separately).

---

# 10. Final notes / practical checklist for you to run now

1. **Run the feature alignment code (Section 1).** Confirm `missing = set(expected) - set(df_ext.columns)` is empty after alignment. If not, update your feature generator to always output canonical columns (including month dummies and lags). Save canonical list to JSON:

```python
with open(os.path.join(MODELS_DIR, "expected_features.json"), "w") as f:
    json.dump(expected, f, indent=2)
```

2. **Validate selected pipeline** with TimeSeriesSplit (Section 3). If CV stable → good.
3. **Perform residual diagnostics** (Section 4). If residuals autocorrelated → add more temporal features (STL, Fourier, longer lags).
4. **Run ARIMA baseline** (Section 5) and Prophet (Section 6) — compare RMSE/R². Include all three models in your report.
5. **Retrain final model on full data** and produce final 12-month forecasts (Section 7). Save `results/forecasts_final.csv`.
6. **If you want to improve accuracy**: try LightGBM / XGBoost / CatBoost + feature selection (Boruta or SHAP) + time-series CV + ensemble stacking. Use Optuna to tune hyperparams.

---

If you want I can now:

* produce the **single notebook cell** that runs Steps 1→4 (alignment, quick CV on existing saved pipeline, residual checks) so you can run it end-to-end, **or**
* produce a ready-to-run script that retrains final model and writes `results/forecasts_final.csv` using your best pipeline (LGB or pipeline if you can re-fit it).

Which one do you want me to output next (one notebook cell to run checks now, or the retrain+forecast script)?
